\chapter*{Conclusions}
\addcontentsline{toc}{chapter}{Conclusion}
\label{chap:IV-0-conclusions}

  The work exposed in this PhD thesis details the contribution of the author to the development, testing, and characterization of the DAQ system of the Triple-GEM upgrade project for the muon spectrometer of CMS. \\

  The Triple-GEM upgrade project aims at improving the performance of the muon spectrometer of CMS which will suffer from the increase in luminosity of the LHC in the coming years. The installation of the GE1/1 stations has been approved by the collaboration and will occur during LS2. To prepare for the integration in CMS, a small scale test will take place during the YETS 2016 involving the instrumentation of four superchambers in one endcap of the detector. \\

  In the CMS GEM collaboration, the GEM DAQ group focused on the design of the DAQ system of the detectors. Within this group, we were in charge of the firmware design of the on-detector and off-detector electronics, namely the OptoHybrid and GLIB boards. We developed both a flexible firmware architecture which provides the user with numerous monitoring and control options, and a web application which allows the user to control the system dynamically. After a successful integration of the components within a stable system, they were tested during two test beam campaigns in November 2014 and November 2015. These demonstrated that the designed DAQ system is able to handle a superchamber of GE1/1 detectors with ease. Furthermore, they enabled the recording of data that yielded various measurements of the efficiency and other significant parameters of the detector. During the analysis of the data, we performed various studies of the noise and efficiency levels against high-voltage, threshold, and particle rates. We obtained a single chamber efficiency of 97\% and tested the rate capability up until a trigger rate of 120 kHz with a resulting efficiency of 96\%. The analysis we have performed show that the GEM detectors equipped with the DAQ system we have designed meet the requirements of the project regarding the efficiency and the rate capability of the chambers. \\

  After the DAQ system had been tested extensively, we created an automated method designed to qualify the electronic components to be used in CMS. For this, we developed a data analysis tool which, for the first time, makes use of the full extend of the VFAT2 capabilities to perform the bias and analysis of the analog front-end. The procedure is used to detect faulty units, and characterize and align the analog response of the chip channel by channel. Furthermore, we designed a GEB testing board to test each position on the GEB and detect broken lines. The board relies on an FPGA coupled with an MCU to communicate with the OptoHybrid and test the integrity of the transmitted signals. Finally, the system as a whole is tested using a script which targets specific components of the architectures. Random read/write operations to the GLIB, OptoHybrid, and VFAT2s are performed, tracking data is read out, and stress tests are done to push the system to the limit. The tools we created are used for the preparation of the integration test to select appropriate components and install testing facilities at CERN and in other associated research laboratories. \\

  Finally, to characterize the behavior of the on-detector electronics when subject to radiation, we exposed two OptoHybrid v2a boards to proton beams during irradiation tests. For this, we developed custom firmware for the FPGAs in order to compute the interaction cross section with the various components inside the chip and to study the effectiveness of mitigation techniques. After analysis of the data, we obtained an interaction cross section of 3.08 $ \times $ 10$^{-7}$ cm$^{2}$ and 1.02 $ \times $ 10$^{-7}$ cm$^{2}$ for the configuration memory of the FPGA and the BRAM respectively, the two main sources of errors. When transposed to the environment of CMS, this yields a rate of 27 errors a day and 9 errors a day respectively. These errors can be mitigated using triplication techniques which we have proven to be efficient at low particle fluxes. Over the course of the tests, the FPGAs were exposed to a total of 84 kRad, corresponding to a dose 8 times higher than what will be collected during the totality of the LHC Phase 2. We did not observe any increase in the interaction cross section for the components of the FPGAs which continued to function correctly until the end of the tests. \\

  All the developments and studies performed on the DAQ system of the Triple-GEM upgrade project have allowed for the design and characterization of a system well suited for installation in CMS from the ground up. The system has been integrated and tested extensively under various conditions and proven to be stable and maintain efficiency in a harsh environments. Along with the firmware developments, we also created software which allows users to monitor and control the system to perform a wide range of tests on the detectors. Finally, the various data analysis have yielded results on the parameters of the detectors as well as the response of the electronics to signals and radiation. These studies are critical to ensure good operation during data taking in CMS.



  Along with the advancements in microelectronics and the rise of new technologies, the architecture of DAQ systems will evolve to become more efficient and data driven. This is illustrated through the work we performed on the design of two DAQ systems: one controlling a small Triple-GEM prototype which followed the classical rules of DAQ design, and one that made use of novel technologies to create an innovative architecture. \\

  The two systems implement radically different topologies to interconnect their nodes and handle the flow of data. The readout system of the 10 cm $ \times $ 10 cm GEM prototype, which evolved over time according to the readiness of the hardware components, uses a star network to control the various subsystems. In this configuration, data is pulled from node to node and systematically transits through the central computing node which becomes the bottleneck of the system. The DAQ system relies on a updating scheme with a high frequency of pull requests in order to remain up to date. This in turn increases the traffic on the network and impacts the speed of the communications while diminishing the allocated bandwidth. \\

  The second system implements a two-way mesh network based on novel web technologies which allows for event driven functionalities. Helped by the increase in computational power of microelectronics, the system merges the functions of the central computing node and front-end electronics in a single device which runs a web server. The latter delivers the control and monitoring application to the client and pushes the data in real-time. Furthermore, a direct connection between the server or client and the database can be established to store information. \\

  The architecture created to develop this proof-of-concept has shown to bring improvements on various fronts of data and system management. It provides a truly event-driven architecture which renders obsolete the need of a constant pull mechanism to ensure that all systems are up to date. This in turn reduces the traffic on the network with response times up to ten times faster. The architecture also limits the number of subsystems in the DAQ system and thus maintenance by merging the functionalities of various components together. Finally, relying on a web application to control the system ensures a wider compatibility and lower maintenance cost than system dependent applications.
